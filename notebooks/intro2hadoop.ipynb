{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Hadoop\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "----- \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this Notebook, we will introduce Hadoop and the Hadoop Distributed\n",
    "File System, which underlie the entire Hadoop ecosystem. Our setup will\n",
    "be using a single Hadoop node, which will not be very fast, especially\n",
    "when compared to simply running Python code directly. However, even with\n",
    "this simple setup, the full Hadoop process will be demonstrated,\n",
    "including the use of the Hadoop file system (HDFS) and the Hadoop\n",
    "Streaming process model.\n",
    "\n",
    "Typically, Hadoop is operated on a large cluster that runs both Hadoop\n",
    "and HDFS, although with the development of Yarn, more diverse workflows\n",
    "are now possible. In this Notebook, we only explore the basic Hadoop\n",
    "components of Hadoop and HDFS, which work together to run code on the\n",
    "nodes that hold the relevant data in order to maximize throughput.\n",
    "[Other resources][hort] exist to learn more about Yarn and other Hadoop\n",
    "workflows, and we will explore using [Pig][nb3] this week, and using\n",
    "Spark in Week 14.\n",
    "\n",
    "In the first code cell, we simple test the Hadoop is working in our\n",
    "course [Docker container][info-hadoop]. We do this by displaying the\n",
    "contents of the most recent Hadoop log files. If the files don't exist,\n",
    "or do not have current timestamps, you will need to explore whether\n",
    "Hadoop is working correctly before proceeding through the rest of this\n",
    "Notebook.\n",
    "\n",
    "-----\n",
    "[hort]: http://hortonworks.com/hadoop-tutorial/introducing-apache-hadoop-developers/\n",
    "[wcp]: https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\n",
    "\n",
    "[nb3]: intro2pig.ipynb\n",
    "\n",
    "[info-hadoop]: https://github.com/UI-DataScience/docker-info490/tree/master/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Out File #####\n",
      "ulimit -a for user data_scientist\n",
      "core file size          (blocks, -c) 0\n",
      "data seg size           (kbytes, -d) unlimited\n",
      "scheduling priority             (-e) 0\n",
      "file size               (blocks, -f) unlimited\n",
      "pending signals                 (-i) 7764\n",
      "max locked memory       (kbytes, -l) 82000\n",
      "max memory size         (kbytes, -m) unlimited\n",
      "open files                      (-n) 1048576\n",
      "pipe size            (512 bytes, -p) 8\n",
      "POSIX message queues     (bytes, -q) 819200\n",
      "real-time priority              (-r) 0\n",
      "stack size              (kbytes, -s) 8192\n",
      "cpu time               (seconds, -t) unlimited\n",
      "max user processes              (-u) unlimited\n",
      "virtual memory          (kbytes, -v) unlimited\n",
      "file locks                      (-x) unlimited\n",
      "\n",
      "##### Log File #####\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1084)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:979)\n",
      "2017-03-30 00:36:28,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 08baa1000667/172.17.0.2:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "2017-03-30 00:36:28,789 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM\n",
      "2017-03-30 00:36:28,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down DataNode at 08baa1000667/172.17.0.2\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "echo '##### Out File #####'\n",
    "out_file=$(ls -la /usr/local/hadoop/logs/hadoop-data*.out | head -1 | awk '{print $9}')\n",
    "cat  $out_file\n",
    "\n",
    "echo\n",
    "echo '##### Log File #####'\n",
    "log_file=$(ls -la /usr/local/hadoop/logs/hadoop-data*.log | head -1 | awk '{print $9}')\n",
    "tail -10  $log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Setup Local Hadoop Environment\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [08baa1000667]\n",
      "08baa1000667: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n",
      "rm: cannot remove ‘/tmp/hsperfdata_root’: Operation not permitted\n",
      "Formatting using clusterid: CID-c876131c-0265-40f6-804f-cf1ad677da1b\n"
     ]
    }
   ],
   "source": [
    "# make sure we stop the namenode and datanodes if there are any running from previous run\n",
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh\n",
    "\n",
    "# Clean up temp files if there are any created during the previous Hadoop operation.\n",
    "!rm -rf /tmp/*\n",
    "\n",
    "# Format the namenode and delete all files in our HDFS.\n",
    "!echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [08baa1000667]\n",
      "08baa1000667: starting namenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-namenode-08baa1000667.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-datanode-08baa1000667.out\n"
     ]
    }
   ],
   "source": [
    "# Restart namenode and datanodes\n",
    "!$HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "!$HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sometimes when the namenode is restarted, it enteres Safe Mode, \n",
    "# not allowing any changes to the file system. \n",
    "# We do want to make changes, so we manually leave Safe Mode.\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/$NB_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HDFS\n",
    "\n",
    "Next, we need to move our data to process into the Hadoop Distributed\n",
    "File system, or HDFS. HDFS is a a file system that is designed to work\n",
    "effectively with the Hadoop environment. In a typical Hadoop cluster,\n",
    "files would be broken up and distributed to different Hadoop nodes. The\n",
    "processing is moved to the data in this model, which can produce high\n",
    "throughput, especially for map/reduce programming tasks. However, this\n",
    "means you can not simply move around the HDFS file system in the same\n",
    "manner as a traditional Unix file system, since the components of a\n",
    "particular file are not all collocated. Instead, we must use the [HDFS\n",
    "file system interface][hdfs], which is invoked by using\n",
    "`$HADOOP_PREFIX/bin/hdfs`. Running this command by itself in our Hadoop\n",
    "Docker container will list the available commands, as shown in the\n",
    "following code cell.\n",
    "\n",
    "-----\n",
    "\n",
    "[hdfs]: https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The standard command we will use is `dfs` which runs a filesystem\n",
    "command on the HDFS file system that is supported by Hadoop. The [list\n",
    "of supported `dfs` commands][dfsl] is extensive, and mirrors many of the\n",
    "traditional Unix file systems commands. The full listing can be obtained\n",
    "by entering `$HADOOP_PREFIX/bin/hdfs dfs` at the prompt in our Hadoop\n",
    "Docker container, as shown below.\n",
    "\n",
    "-----\n",
    "\n",
    "[dfsl]: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    " Some of the more useful commands for this class\n",
    "include:\n",
    "\n",
    "- `-cat`: copies the source path to STDOUT.\n",
    "\n",
    "- `-count -h`: counts the number of directories, files and byts under the\n",
    "path specified. With the `-h` flag, the output is displayed in a\n",
    "human-readable format.\n",
    "\n",
    "- `-expunge`: empties the trash. By default, files and directories are\n",
    "not removed from HDFS with the `rm` command, they are simply moved to the\n",
    "trash. This can be useful when HDFS supplies a `Name node is in safe\n",
    "mode.` message. \n",
    "\n",
    "- `-ls`: lists the contents of the indicated directory in HDFS.\n",
    "\n",
    "- `-mkdir -p`: creates a new directory in HDFS at the specified\n",
    "location. With the `-p` flag any parent directory specified in the full\n",
    "path will also be created as necessary.\n",
    "\n",
    "- `-put`: copies indicated file(s) from local host file system into the\n",
    "specified path in HDFS.\n",
    "\n",
    "- `-rm -f -r`: delete the indicated file or directory. With the `-r -f`\n",
    "flags, the command will not display any message and any will delete any\n",
    "files or directories under the indicated directory. The `-skipTrash`\n",
    "flag should be used to delete the indicated resource immediately.\n",
    "\n",
    "- `-tail`: display the last kilobyte of the indicated file to STDOUT.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display HDFS root directory structure\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display HDFS directory\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not a local filesystem directory so we get an error\n",
    "!ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Free Space\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Hadoop Example\n",
    "\n",
    "We can now turn to a complete Hadoop example. This example will run the\n",
    "`grep` command over a set of input files to search for the occurrences of\n",
    "a particular regular expression, which in this case is the three\n",
    "character sequence _dfs_ followed by one or more lowercase characters or\n",
    "a period. Hadoop tasks read inout from the HDFS filesystem and write\n",
    "their output to the HDFS file system. Thus we need to create an input\n",
    "directory, move our data to this input directory, before we can execute\n",
    "our Hadoop task. \n",
    "\n",
    "As demonstrated in the following code cell, we can do this easily with\n",
    "HDFS commands, after which we execute our specific Hadoop task. Notice\n",
    "how we include the input and output directories as part of the task\n",
    "execution. In some cases these will be indicated by flags. Finally, we\n",
    "display the directory contents, which demonstrate the successful\n",
    "completion of this task. Since Hadoop tasks can involve complex\n",
    "operations over a distributed file system, Hadoop tasks, by default,\n",
    "display a considerably quantity of information. You can capture the\n",
    "STDERR of any Hadoop (or HDFS) command to hide these informational\n",
    "messages. Of course, this will also hide any error messages, so proceed\n",
    "carefully if employing this technique.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example derived from Hadoop single node setup documentation:\n",
    "#\n",
    "# https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html\n",
    "#\n",
    "\n",
    "# Remove old directory (if it exsits) to have clean example\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f hadoop\n",
    "\n",
    "# Make directorties for example application\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p hadoop\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p hadoop/input\n",
    "\n",
    "# Copy data into example input directory\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/*.xml hadoop/input\n",
    "\n",
    "# Running Hadoop example to test installation\n",
    "!$HADOOP_PREFIX/bin/hadoop jar \\\n",
    "    $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar \\\n",
    "    grep hadoop/input hadoop/output 'dfs[a-z.]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display directory heirarchy\n",
    "\n",
    "!echo\n",
    "!echo 'Hadoop Directory'\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls hadoop/\n",
    "\n",
    "!echo\n",
    "!echo 'Hadoop Input Directory'\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls hadoop/input\n",
    "\n",
    "!echo\n",
    "!echo 'Hadoop Output Directory'\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls hadoop/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "As shown in the output directory display, several files were created by\n",
    "our Hadoop task. The first file, `_SUCCESS`, is self-explanatory,\n",
    "especially when you see the file is empty. The second file,\n",
    "`part-r-00000` contains the output of the command. We can display the\n",
    "contents of this file by using the HDFS `cat` command as shown in the\n",
    "following cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -cat hadoop/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Of course, we want to know if this output is correct. To test this, we\n",
    "can use standard Unix `grep` command to find the expected output, which\n",
    "shows the same four lines (albeit we don't use a full regular\n",
    "expression, so the whole line is displayed by default).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!grep --color 'dfs[a-z.]' $HADOOP_PREFIX/etc/hadoop/*.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Acquiring Data \n",
    "\n",
    "In the next two lessons, we will analyze text data by using Hadoop\n",
    "map-reduce and Pig. As a result, we finish this Notebook by acquiring a\n",
    "text data set for later analysis, which we stage locally (i.e., outside\n",
    "HDFS). First, we delete our local directory if it exists and create it\n",
    "to have a clean install.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "# A Bash Shell Script to delete the Hadoop diorectory if it exists, afterwhich\n",
    "# make a new Hadoop directory\n",
    "\n",
    "# Our directory name\n",
    "DIR=$HOME/hadoop\n",
    "\n",
    "# Delete if exists\n",
    "if [ -d \"$DIR\" ]; then\n",
    "    rm -rf \"$DIR\"\n",
    "fi\n",
    "\n",
    "# Now make the directory\n",
    "mkdir \"$DIR\"\n",
    "\n",
    "ls -la $DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Acquiring Data\n",
    "\n",
    "To perform data analysis by using Hadoop, we will need a data set. In the\n",
    "Notebook for the second lesson this week, we will perform a simple\n",
    "map/reduce operation that will require text data to operate. While there\n",
    "are a number of possible options, for this example we can grab a free\n",
    "book from [Project Gutenberg][pg]:\n",
    "\n",
    "    wget --directory-prefix=$HOME/hadoop/ --output-document=book.txt \\\n",
    "        http://www.gutenberg.org/cache/epub/4300/pg4300.txt`\n",
    "\n",
    "In this case, we have grabbed the full text of the novel _Ulysses_, by\n",
    "James Joyce, and placed the text in the `hadoop` subdirectory of our\n",
    "_home_ directory.\n",
    "\n",
    "-----\n",
    "[pg]: http://www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab a book to process\n",
    "!wget --output-document=$HOME/hadoop/book.txt \\\n",
    "http://www.gutenberg.org/files/4300/4300-0.txt\n",
    "\n",
    "# On Course JupyterServer we simply copy the text from the data directory\n",
    "# Since the Gutenberg site would otherwise think the students were launching a\n",
    "# denial of service attack as they would all come from the same site.\n",
    "\n",
    "#!cp /home/data_scientist/data/book.txt $HOME/hadoop/book.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "At this point, we first need to create an directory to hold the input\n",
    "and output of our Hadoop task. We will create a new directory called\n",
    "`wc` with a subdirectory called `in` to hold the input data for our\n",
    "Hadoop task. Second, we will need to copy the book text file into this\n",
    "new HDFS directory. This means we will need to run the following two\n",
    "Hadoop commands:\n",
    "\n",
    "1. `$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in`\n",
    "2. `$HADOOP_PREFIX/bin/hdfs dfs -put book.txt wc/in/book.txt`\n",
    "\n",
    "The following cell contains these commands (and two other commands) the\n",
    "result of running these two commands, as well as the `dfs -ls` command\n",
    "to display the contents of our new HDFS directory, and the `dfs -count`\n",
    "command to show the size of the directory contents. At the end of this\n",
    "output will be a message from Hadoop, which simply states that files are\n",
    "being immediately deleted. This value can be changed to cache files\n",
    "before deleting for a specific time interval, which would, of course,\n",
    "allow files to be recovered if accidentally deleted.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc\n",
    "\n",
    "!echo\n",
    "!echo 'Creating input directory, and copying data.'\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -put $HOME/hadoop/book.txt wc/in/book.txt\n",
    "\n",
    "!echo\n",
    "!echo 'Input directory contents'\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -count -h wc/in/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Having the namenode and datanodes running in the background consumes quite a bit of memory. So I think we should shut down the nodes at the end of the notebook:\n",
    "\n",
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Hadoop and the HDFS file system.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Create a new Hadoop HDFS directory, use your own name for the\n",
    "directory name.\n",
    "2. Copy one or more local files into your new Hadoop directory. Run a\n",
    "Hadoop command to display the files and their byte count, do the results\n",
    "agree with your local values?\n",
    "3. Run a different `grep` example on the book you downloaded. Do the\n",
    "results make sense?\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
